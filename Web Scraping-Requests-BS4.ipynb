{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules used in this notebook\n",
    "import bs4\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import pprint as pp\n",
    "from IPython.core.display import display, HTML, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping using Requests and Beautiful Soup\n",
    "(A significant portion of this material was adapted from Dr. Nick Freeman's workshop on Web-scraping and from Dr. Gregory Bott's Python Data Acquisition Course).a simple quotes siteF fre, and from Wikipedia.\n",
    "\n",
    "In this notebook, we will see how we can use Python to harvest data from HTML. In particular, we will be using the python `requests` and `beautifulsoup` libraries to request and parse financial data from a simple quotes site, from Yahoo Finance, and from Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Scrape?\n",
    "Sometimes the data you require is not available in a structured, downloadable format. Often the most current data is available only on a web site. This notebook demonstrates how to \"scrape\" data from web sites using several different methods.\n",
    "\n",
    "> If you understand how to web scrape, any available data on the web is a database for you!!\n",
    "\n",
    "> **Should I use RegeX (regular expressions) to parse web data?**\n",
    "> Although extracting patterned data from a text file is directly in the wheelhouse of RegEx, I recommend *against* using it to parse HTML. Crafting an expression that returns all desired strings while excluding all undesired strings is likely to fail. HTML pages vary widely and will return or exclude data in ways you cannot anticipate. Instead, use a library such as Beautiful Soup, to parse HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Scraping\n",
    "Scraping a web site is not always the best option. Consider the following questions prior to scraping:\n",
    "1. Does the web site in question allow scraping? (check the robots.txt)\n",
    "2. Am I able to adhere to the requests of the web site (again, see robots.txt)?\n",
    "3. Is scraping the easiest, most efficient, or most reliable method? Copy/paste? API? Download data file?\n",
    "4. Is it ethical to scrape this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Knowledge\n",
    "Although it's not necessary to be an HTML expert, having a strong grasp of how HTML works is helpful if you intend to extract information from a web page.\n",
    "\n",
    "There are many HTML tutorials available, but one of the most concise options is W3Schools (https://www.w3schools.com/html/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Source HTML\n",
    "To determine the best method for extracting data from an HTML page, you must view the HTML. To view HTML using Firefox or Chrome, right-click the page and then click **View page source** (to view the entire page as a flat file) or click **Inspect** (to view an HTML inspection tool to navigate the page elements).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status codes\n",
    "Use HTML status codes to better understand how a web server is responding to a request and to take action based on the response. Below is a graphic of HTML status codes.\n",
    "\n",
    "![](images\\HTTP_status_codes.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requests\n",
    "We'll use the `requests` library to obtain status codes, headers, and content from a web server and then use `beautifulsoup` to make sense of it. First, a quick review of how web browsers use HTTP to obtain information via request-response. When you type a URL into your browser, the browser makes a request from the destination web server and receives a response consisting of headers, a status code, and the content.\n",
    "\n",
    "![](images\\request_response.png)\n",
    "\n",
    "To install requests, use conda:\n",
    "```Python\n",
    "conda install requests\n",
    "```\n",
    "After you have imported `requests`, use it to retrieve content such as the university's home page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML status code: <Response [200]> \n",
      "\n",
      "<!DOCTYPE html>\n",
      "                <html lang=\"en\">\n",
      "                    <head>\n",
      "                        <meta charset=\"utf-8\">\n",
      "                        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
      "                        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "                        <meta name=\"description\" content=\"Official Web Site of The University of Alabama. Founded in 1831 as the state&apos;s flagship university, UA is a student-centered research university and academic community united in its commitment to enhancing the quality of life for all Alabamians.\">\n",
      "                        <meta property=\"og:title\" content=\"The University of Alabama\" />\n",
      "                        <meta property=\"og:type\" conte\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('https://www.ua.edu')\n",
    "\n",
    "# Print the HTML response code given by the server\n",
    "print(\"HTML status code:\",r,\"\\n\")\n",
    "\n",
    "# Print the first 750 characters of the text returned\n",
    "print(r.text[:750])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical Scraping\n",
    "Although the \"rights\" to scrape publicly available data is debatable, we will not have that debate in our class. Instead, we will respect the wishes of those providing the data. If and how a site should be scraped should be communicated in a file called `robots.txt` and located at the root of the site.\n",
    "\n",
    "Below is the Robots.txt file for the university's web site. It disallows several locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-agent: *\n",
      "Disallow: /api\n",
      "Disallow: /cache\n",
      "Disallow: /docs\n",
      "Disallow: /inc\n",
      "Disallow: /templates\n",
      "Disallow: /Templates\n",
      "Disallow: /checkCache\n",
      "Disallow: /imagecache\n",
      "Disallow: /academics/catalogs/catalog02\n",
      "Disallow: /academics/catalogs/catalog04\n",
      "Disallow: /academics/catalogs/catalog06\n",
      "Disallow: /academics/catalogs/catalog08\n",
      "Disallow: /academics/catalogs/catalog10\n"
     ]
    }
   ],
   "source": [
    "# UA.edu example using error handling\n",
    "try:\n",
    "    r = requests.get('https://www.ua.edu/robots.txt')\n",
    "    r.raise_for_status()  # Raises a HTTPError if the status is 4xx, 5xxx\n",
    "except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n",
    "    print(\"No response from server\")\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen `requests` in action, to get a better understanding of http requests, we will first target the website `httpbin.org`. The same person who developed `requests` and `requests-html` also created `httpbin.org`. The site enables developers to test their requests before deploying applications. The following code block: \n",
    "1. creates a variable named `target_url` that points an area of `httpbin.org` that allows users to test **GET** requests, \n",
    "2. makes a **GET** request using the `get` method available in the python requests package and stores the response in a variable named `r`, and \n",
    "3. prints the content of the `r` object in *JavaScript Object Notation* format (json). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args': {},\n",
      " 'headers': {'Accept': '*/*',\n",
      "             'Accept-Encoding': 'gzip, deflate',\n",
      "             'Host': 'httpbin.org',\n",
      "             'User-Agent': 'python-requests/2.25.1',\n",
      "             'X-Amzn-Trace-Id': 'Root=1-60c75e5d-62e800b86f2fdff435d3300a'},\n",
      " 'origin': '68.62.227.75',\n",
      " 'url': 'http://httpbin.org/get'}\n"
     ]
    }
   ],
   "source": [
    "# 1) specify target_url \n",
    "target_url = 'http://httpbin.org/get'\n",
    "\n",
    "# 2) make request\n",
    "r = requests.get(target_url)\n",
    "\n",
    "# 3) print response as json\n",
    "pp.pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Agent and Referer Headers\n",
    "Note that the json representation returned is a python dictionary. Inspecting the dictionary, we can see that there are keys for `args`, `headers`, `origin`, and `url`. The values in these keys give us an idea of some of the information that we transmit when making http requests. Moreover, we can control this information to some degree. To demonstrate this, we will look into how we can modify the headers that we send with a request. Specifically, we will modify our `User-Agent` and add a referer. Information on valid http request headers can be found at https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers. \n",
    "\n",
    "Before we demonstrate how to modify the user agent and referer headers, let's understand the role that they play in an http request:\n",
    "- `User-Agent`: Contains a characteristic string that allows the network protocol peers to **identify the application type, operating system, software vendor or software version of the requesting software user agent**. \n",
    "- `Referer`: The address of the previous web page from which a link to the currently requested page was followed.\n",
    "\n",
    "Note that a website that we are making a request to can use the two headers we are considering to determine if we are a real user or a computer program. Specifically, as we can see in the response we received earlier, the default User-Agent used by the requests package show that we are making the request using the package. A website wishing to deter programmatic access can easily detect and deny such requests. Also, a request may seem more realistic if we are referred from a search engine such as Google. To modify these headers, we need to pass a dictionary of headers when we make a request. The following code demonstrates how this can be done. Specifically, we:\n",
    "\n",
    "1. Define a variable named `my_user_agent`, which stores a string with a realistic value, \n",
    "2. Define a dictionary object named `headers`,\n",
    "3. Add the defined user-agent variable, \n",
    "4. Specify a `Referer` header that suggests we were referred, i.e., made the request from, the Google search engine.\n",
    "5. Make the same request as before with our new headers, and\n",
    "6. Print the response as json.\n",
    "\n",
    "The printed output below shows that our headers were correctly modified. Moreover, our request will now look more realistic to a target website. We will now look at how to pass parameters with a request. This is very common when working with web APIs, where the parameters are used to filter the data returned by the request and, oftentimes, to authenticate users. Similar to how we specified headers, we can specify parameters by passing a dictionary of parameters when we make the request. The following code block demonstrates this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args': {},\n",
      " 'headers': {'Accept': '*/*',\n",
      "             'Accept-Encoding': 'gzip, deflate',\n",
      "             'Host': 'httpbin.org',\n",
      "             'Referer': 'https://www.google.com',\n",
      "             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) '\n",
      "                           'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
      "                           'Chrome/79.0.3945.130 Safari/537.36',\n",
      "             'X-Amzn-Trace-Id': 'Root=1-60c75e74-4b4187e771867f194f25d984'},\n",
      " 'origin': '68.62.227.75',\n",
      " 'url': 'http://httpbin.org/get'}\n"
     ]
    }
   ],
   "source": [
    "my_user_agent = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "\n",
    "my_headers = {'User-Agent': my_user_agent, \n",
    "             'Referer': 'https://www.google.com'}\n",
    "\n",
    "r = requests.get(target_url, headers = my_headers)\n",
    "\n",
    "pp.pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args': {'param1': 'my_param_value'},\n",
      " 'headers': {'Accept': '*/*',\n",
      "             'Accept-Encoding': 'gzip, deflate',\n",
      "             'Host': 'httpbin.org',\n",
      "             'Referer': 'https://www.google.com',\n",
      "             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) '\n",
      "                           'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
      "                           'Chrome/79.0.3945.130 Safari/537.36',\n",
      "             'X-Amzn-Trace-Id': 'Root=1-60c75e7d-4230e5aa7dc6b28c5c46a8a4'},\n",
      " 'origin': '68.62.227.75',\n",
      " 'url': 'http://httpbin.org/get?param1=my_param_value'}\n"
     ]
    }
   ],
   "source": [
    "# Define a test parameter, param1\n",
    "my_params = {'param1': 'my_param_value'}\n",
    "\n",
    "# Make the request, passing headers and parameters\n",
    "r = requests.get(target_url, \n",
    "                 headers = my_headers, \n",
    "                 params = my_params)\n",
    "\n",
    "# Print the response as json\n",
    "pp.pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the response output includes our parameters as `args`. Also notice that the `url` value has been updated. In particular, the string `?param1=my_param_value` was appended to the end of our `target_url`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Scraping - Quote of the Day\n",
    "Start by navigating to the page that you want to scrape and obtaining its URL. For this example, we want to scrape the quoteof the day from WisdomQuotes.com. The URL is: http://wisdomquotes.com/quote-of-the-day/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://wisdomquotes.com/quote-of-the-day/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supply a header to the web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b\\'<!DOCTYPE html><html lang=\"en-US\" prefix=\"og: '\n",
      " 'https://ogp.me/ns#\"><head>\\\\n<script async '\n",
      " 'src=\"https://www.googletagmanager.com/gtag/js?id=UA-6742173-29\"></script> '\n",
      " '<script '\n",
      " 'src=\"data:text/javascript;base64,d2luZG93LmRhdGFMYXllcj13aW5kb3cuZGF0YUxheWVyfHxbXTtmdW5jdGlvbiBndGFnKCl7ZGF0YUxheWVyLnB1c2goYXJndW1lbnRzKTt9Cmd0YWcoJ2pzJyxuZXcgRGF0ZSgpKTtndGFnKCdjb25maWcnLCdVQS02NzQyMTczLTI5Jyk7\" '\n",
      " 'defer></script> <meta charset=\"UTF-8\"><meta '\n",
      " 'http-equiv=\"x-dns-prefetch-control\" content=\"on\"><meta name=\"viewport\" '\n",
      " 'content=\"width=device-width, initial-scale=1\"><meta name=\"msvalidate.01\" '\n",
      " 'content=\"18DDC7EE000F2D2304FCFD71B1F9B110\" /><link rel=\"profile\" '\n",
      " 'href=\"https://gmpg.org/xfn/11\"><link rel=\"pingback\" '\n",
      " 'href=\"https://wisdomquotes.com/xmlrpc.php\"><link rel=\"apple-touch-icon\" '\n",
      " 'sizes=\"180x180\" href=\"/apple-touch-icon.png\"><link rel=\"icon\" '\n",
      " 'type=\"image/png\" sizes=\"32x32\" href=\"/favicon-32x32.png\"><link rel=\"icon\" '\n",
      " 'type=\"image/png\" sizes=\"16x16\" href=\"/favicon-16x16.png\"><link '\n",
      " 'rel=\"manifest\" href=\"/manifest.json\">')\n"
     ]
    }
   ],
   "source": [
    "# Unless you send a header, a web server may reject your request\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "\n",
    "#Request page, send headers\n",
    "r = requests.get(url, headers=headers)\n",
    "\n",
    "pp.pprint(str(r.content)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup\n",
    "The idea of 'soup' is unordered stuff in a container (aka a soup). Specifically, it refers to malformed tags. The term apparently originated from Tag Soup, an interpreter that was able to handle messy HTML/SGML markup, much like Python's Beautiful Soup.\n",
    "\n",
    "We'll use `requests` to obtain HTML, check status, send headers, etc., and then use Beautiful Soup (imported as `bs4`) to make sense of, or parse the HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Beautiful Soup\n",
    "To install the beautiful soup package (bs4) use pip.\n",
    "```\n",
    "conda install -c conda-forge bs4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, inpsect the HTML to find a unique method to identify the information you wish to extract.\n",
    "\n",
    "All quotes are prefaced with \"blockquote\", which makes it easy to filter all the quotes on the page.\n",
    "\n",
    "Use the ```find_all()``` method to return a list of quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Quote of the Day - Wisdom Quotes</title>\n",
      "Quote of the Day - Wisdom Quotes\n"
     ]
    }
   ],
   "source": [
    "# Use .text to return string (.content returns bytes, .text returns string)\n",
    "soup = bs4.BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "# Print the page title to confirm we have successfully parsed the web page\n",
    "print(soup.title)\n",
    "\n",
    "# Print only the text (no tags)\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the HTML to find a consistent (and hopefully unique) \n",
    "#   pattern for the quotes\n",
    "quotes = soup.find_all(\"blockquote\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The find_all method returns a result set, which can be used like a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n"
     ]
    }
   ],
   "source": [
    "print(type(quotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<blockquote><p>Somewhere, something incredible is waiting to be known. Sharon Begley</p></blockquote>\n"
     ]
    }
   ],
   "source": [
    "# Print the 5th quote\n",
    "print(quotes[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men must live and create. Live to the point of tears. Albert Camus\n"
     ]
    }
   ],
   "source": [
    "print(quotes[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the list of quotes and print the text attribute of the element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "\n",
      "-----------------------------------\n",
      "Hard choices, easy life. Easy choices, hard life. Jerzy Gregory Click to tweet\n",
      "-----------------------------------\n",
      "Men must live and create. Live to the point of tears. Albert Camus\n",
      "-----------------------------------\n",
      "Great hopes make great men. Thomas Fuller\n",
      "-----------------------------------\n",
      "Somewhere, something incredible is waiting to be known. Sharon Begley\n",
      "-----------------------------------\n",
      "Memories of our lives, of our works and our deeds will continue in others. Rosa Parks\n",
      "-----------------------------------\n",
      "Nobody in life gets exactly what they thought they were going to get. But if you work really hard and you’re kind, amazing things will happen. Conan O’Brien\n",
      "-----------------------------------\n",
      "The past cannot be changed. The future is yet in your power. Mary Pickford\n",
      "-----------------------------------\n",
      "Persistence overshadows even talent as the most valuable resource shaping the quality of life. Tony Robbins\n",
      "-----------------------------------\n",
      "I don’t dream at night, I dream all day; I dream for a living. Steven Spielberg\n"
     ]
    }
   ],
   "source": [
    "for quote in quotes[:10]:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(quote.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Farts are like children, I’m proud of mine and disgusted by yours. Bill Murray\n",
      "-----------------------------------\n",
      "While we try to teach our children all about life, our children teach us what life is all about. Angela Schwindt\n"
     ]
    }
   ],
   "source": [
    "for quote in quotes:\n",
    "    if \"children\" in quote.text:\n",
    "        print(\"-----------------------------------\")\n",
    "        print(quote.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find by tag and class\n",
    "Often the information that you need is not labled by tag alone. For example, if you wanted to extract quotes from GoodReads, you could not use tag alone. All the quotes on the page are within div tags, but are uniquely identifed by class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodread_images = soup.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"regret quotes take risks you cant find meaning playing safe maxime lagace wisdom nature\" class=\"aligncenter size-full wp-image-18233\" height=\"510\" src=\"https://wisdomquotes.com/wp-content/uploads/regret-quotes-take-risks-you-cant-find-meaning-by-playing-it-safe-maxime-lagace-wisdom-quotes.jpg\" width=\"420\"/>,\n",
       " <img alt=\"quote of the day inspirational january choices life easy hard jerzy gregory wisdom quotes\" class=\"aligncenter size-full wp-image-5369\" height=\"247\" loading=\"lazy\" sizes=\"(max-width: 420px) 100vw, 420px\" src=\"https://wisdomquotes.com/wp-content/uploads/quote-of-the-day-inspirational-january-hard-choices-easy-life-easy-choices-hard-life-jerzy-gregory-wisdom-quotes.jpg\" srcset=\"https://wisdomquotes.com/wp-content/uploads/quote-of-the-day-inspirational-january-hard-choices-easy-life-easy-choices-hard-life-jerzy-gregory-wisdom-quotes.jpg 420w, https://wisdomquotes.com/wp-content/uploads/quote-of-the-day-inspirational-january-hard-choices-easy-life-easy-choices-hard-life-jerzy-gregory-wisdom-quotes-300x176.jpg 300w\" width=\"420\"/>,\n",
       " <img alt=\"quote of the day it isnt where you came from where you doing that counts ella fitzgerald wisdom nature road\" class=\"aligncenter size-full wp-image-14656\" height=\"260\" loading=\"lazy\" sizes=\"(max-width: 420px) 100vw, 420px\" src=\"https://wisdomquotes.com/wp-content/uploads/quote-of-the-day-it-isnt-where-you-came-from-its-where-you-re-doing-that-counts-ella-fitzgerald-wisdom-quotes.jpg\" srcset=\"https://wisdomquotes.com/wp-content/uploads/quote-of-the-day-it-isnt-where-you-came-from-its-where-you-re-doing-that-counts-ella-fitzgerald-wisdom-quotes.jpg 420w, https://wisdomquotes.com/wp-content/uploads/quote-of-the-day-it-isnt-where-you-came-from-its-where-you-re-doing-that-counts-ella-fitzgerald-wisdom-quotes-300x186.jpg 300w\" width=\"420\"/>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 3 images\n",
    "goodread_images[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://wisdomquotes.com/wp-content/uploads/regret-quotes-take-risks-you-cant-find-meaning-by-playing-it-safe-maxime-lagace-wisdom-quotes.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use attrs property to get src attribute from img tag\n",
    "Image(url=goodread_images[0].attrs['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yahoo Finance Robots.txt file\n",
    "Source: Adapted from Dr. Freeman's Web Scraping Workshop\n",
    "\n",
    "We will be harvesting data from `https://finance.yahoo.com`. So, before we begin scraping, let's first look at the `robots.txt` file published for the site to see if there is any activity that is specifically *disallowed* or if any additional guidelines for access are given (e.g., maximum rate of scraping). The following code block makes a **GET** request for the site's `robots.txt` file and prints the returned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://finance.yahoo.com/robots.txt')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the `robots.txt` file specifies several paths that are disallowed for all user-agents. This is indicated by the `User-agent: *` line. However, also note that the files provides several sitemaps that actually help programs determine the layout of the site. We will use one of these sitemaps to help us access stock information for companies operating in specific sectors. Before doing this, let's store a list of prohibited paths so that we can ensure we do not volate the `robots.txt` directives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prohibited_paths = []\n",
    "for statement in r.text.split('\\n'):\n",
    "    if 'Disallow:' in statement:\n",
    "        current_path = statement.strip('Disallow: ')\n",
    "        prohibited_paths.append('https://finance.yahoo.com' + current_path)\n",
    "prohibited_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block makes a **GET** request for the first sitemap. The response from this request is stored in the variable `r`. We use the `content` attribute of this response to get a *bytes* representation of the reponse and use the *BeautifulSoup* library to convert this data into a `soup` object. We print a *prettified* version of the first 1,000 characters of this object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://finance.yahoo.com/sitemap_en-us_desktop_index.xml')\n",
    "\n",
    "soup = bs4.BeautifulSoup(r.content)\n",
    "\n",
    "#print(soup.prettify()[:1000])\n",
    "pp.pprint(str(soup)[2500:4500]) # Print starting from 2500 characters up to 4500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several URLs of interest that follow the form ` https://finance.yahoo.com/sector/...` e.g., https://finance.yahoo.com/sector/ms_basic_materials. These pages report stock information for top companies operating in the associated sector. The following code block shows how we can use BeautifulSoup to extract all of the `loc` elements that include the word `sector` in the associated text and store them in a list called `sector_urls`. Note how we are insuring that the returned URLS do not match any of the partial strings stored in our list of `prohibited_paths`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prohibited_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_urls = []\n",
    "for loc_tag in soup.find_all('loc'):\n",
    "    if 'sector' in loc_tag.text:\n",
    "        prohibited = False\n",
    "        for path in prohibited_paths:\n",
    "            if path in loc_tag.text:\n",
    "                print('Path prohibited!')\n",
    "                prohibited = True\n",
    "        if not prohibited:\n",
    "            print(f'{loc_tag.text} not prohibited')\n",
    "            sector_urls.append(loc_tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block prints the first URl in our `sector_urls` list. If you visit the site with your web browser, you sill see that the page displays a table of related stocks and associated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_sector_loc = sector_urls[0]\n",
    "print(sector_urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the HTML of the previous URL, you will see that the table is enclosed in a set of `<table>` tags. The following code block \n",
    "1. requests the HTML associated with the URL, \n",
    "2. converts the response to a `soup` object, \n",
    "3. identifies all elements enclosed in `<table>` tags and stores the associated HTML in a list named `tables`, \n",
    "4. prints the number of items in the `tables` object, and\n",
    "5. prints the first 5,000 characters of the first item in the `tables` list.\n",
    "\n",
    "Note how the HTML for the tables uses the `<tr>`, `<th>`, and `<td>` to achieve the desired structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_sector_loc = sector_urls[0]\n",
    "\n",
    "r = requests.get(current_sector_loc)\n",
    "soup = bs4.BeautifulSoup(r.content)\n",
    "\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "print(f'The tables object has {len(tables)} item(s).\\n')\n",
    "\n",
    "# Raw print\n",
    "print(str(tables[0])[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettify printing\n",
    "print(tables[0].prettify()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render as HTML\n",
    "display(HTML(str(tables[0])[:15000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how we can use a loop to iterate over all of the sectors, collecting the data stored in the table and using it to construct a pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for current_sector_loc in sector_urls:\n",
    "    r = requests.get(current_sector_loc)\n",
    "    soup = bs4.BeautifulSoup(r.content)\n",
    "\n",
    "    for current_row in soup.find_all('tr'):\n",
    "        row_list = []\n",
    "        for current_data in current_row.find_all('td'):\n",
    "            row_list.append(current_data.text)\n",
    "        if row_list:\n",
    "            row_list.append(current_sector_loc.split('/')[-1])\n",
    "            row_list.append(datetime.datetime.now().strftime('%m-%d-%Y %H:%M:%S'))\n",
    "            data_list.append(row_list)\n",
    "    \n",
    "columns = ['Symbol', 'Name', 'Price', 'Change', '%Change', \n",
    "           'Volume', 'Avg Vol. (3 mnth)', 'Market Cap', \n",
    "           'PE Ratio', '52 Week Range', 'Sector', 'Scrape Time']\n",
    "\n",
    "all_data = pd.DataFrame(data_list, columns = columns)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia\n",
    "\n",
    "Just to demonstrate a (slightly) more complex harvesting task, let's write some code to collect headlines from Wikipedia pages. As before, let's first check the `robots.txt` file, which is much more extensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://en.wikipedia.org/robots.txt')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block creates a list of prohibited paths for a general web scraping application and displays the first 30 items in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prohibited_paths = []\n",
    "current_user_agent = None\n",
    "for line in r.text.split('\\n'):\n",
    "    if 'User-agent' in line:\n",
    "        current_user_agent = line.strip('User-agent: ')\n",
    "    if current_user_agent is not None:\n",
    "        if current_user_agent.strip() == '*':\n",
    "            if 'Disallow: ' in line:\n",
    "                prohibited_paths.append(line.strip('Disallow: '))\n",
    "                \n",
    "prohibited_paths[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you browse through a few Wikipedia pages, you will notice that the links use a very consitent naming pattern. Suppose, we want to try the phrases in the `words_to_try` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_try = ['Python_Programming', \n",
    "                'Robust_optimization',\n",
    "                'Nonparametric_regression',\n",
    "                'Data_warehouse',\n",
    "                'tHis_wIll_nOt_wErk',\n",
    "                'Integer_programming',\n",
    "                'Sentiment_Analysis']\n",
    "\n",
    "base_url = 'https://en.wikipedia.org/wiki/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code attempts to access the pages defined by our `words_to_try` object and returns a list of the headlines along with the associated word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []\n",
    "\n",
    "for current_word in words_to_try:\n",
    "    current_url = base_url + current_word\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(current_url)\n",
    "        soup = bs4.BeautifulSoup(r.content)\n",
    "\n",
    "        for headline in soup.find_all('span', attrs = {'class':'mw-headline'}):\n",
    "            if 'See also' in headline:\n",
    "                break\n",
    "            else:\n",
    "                headlines.append([headline.text, current_word])\n",
    "    except Exception as e:\n",
    "        print(f'{type(e).__name__} on {current_url}')\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "for row in headlines:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
